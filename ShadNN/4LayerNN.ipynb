{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial hidden weights: [0.95704815 0.89930318] [0.26848811 0.76123365]\n",
      "Initial hidden biases: [0.82601871 0.4513381 ]\n",
      "Initial output weights: [0.7815049] [0.01439598]\n",
      "Initial output biases: [0.39645806]\n"
     ]
    }
   ],
   "source": [
    "def print_one_line(s):\n",
    "    time_string = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "    sys.stdout.write(\"\\r\" + time_string + \" \" + s)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "# Input datasets\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "target = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "epochs = 10000\n",
    "lr = 0.1\n",
    "inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2, 2, 1\n",
    "\n",
    "\n",
    "# Random weights and bias initialization\n",
    "hidden_weights = np.random.uniform(size=(inputLayerNeurons, hiddenLayerNeurons))\n",
    "hidden_bias = np.random.uniform(size=(1, hiddenLayerNeurons))\n",
    "output_weights = np.random.uniform(size=(hiddenLayerNeurons, outputLayerNeurons))\n",
    "output_bias = np.random.uniform(size=(1, outputLayerNeurons))\n",
    "\n",
    "print(\"Initial hidden weights: \", end='')\n",
    "print(*hidden_weights)\n",
    "print(\"Initial hidden biases: \", end='')\n",
    "print(*hidden_bias)\n",
    "print(\"Initial output weights: \", end='')\n",
    "print(*output_weights)\n",
    "print(\"Initial output biases: \", end='')\n",
    "print(*output_bias)\n",
    "\n",
    "print(\"margin\", margin(small_w, small_x[0], labels[0]))\n",
    "\n",
    "print(\"prediction\", prediction(small_w, small_x[0]))\n",
    "print(\"Loss\", LogisticLoss(small_w, small_x[0], labels[0]))\n",
    "print(\"dL\", dLogisticLoss(small_w, small_x[0], labels[0]))\n",
    "\n",
    "print(\"trainLoss\", TrainLoss(small_w, small_x, labels))\n",
    "print(\"dtrainLoss\", dTrainLoss(small_w, small_x, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2, 2, 1\n",
    "\n",
    "# Input datasets\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "target = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(w, x):\n",
    "    w = np.squeeze(np.asarray(w))\n",
    "    x = np.squeeze(np.asarray(x))\n",
    "    return np.dot(w, x)\n",
    "\n",
    "def margin(w, x, y):\n",
    "    return score(w, x) * y\n",
    "\n",
    "def prediction(w, x):\n",
    "    #print(score(w, x))\n",
    "    if score(w, x) > 0:\n",
    "        return -1\n",
    "    if score(w, x) <= 0:\n",
    "        return 1\n",
    "\n",
    "def LogisticLoss(w, x, y):\n",
    "    return np.log(1.0 + 1.0/np.exp(-1.0 * margin(w, x, y)))\n",
    "\n",
    "def dLogisticLoss(w, x, y):\n",
    "    frac = 1.0/(1.0 + np.exp(-margin(w, x, y)))\n",
    "    return y * x * (1.0 - frac)\n",
    "\n",
    "def TrainLoss(w, x, y):\n",
    "    loss = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        loss = np.sum(LogisticLoss(w, x[i], y[i]))\n",
    "    return 1.0/x.shape[0] * loss\n",
    "\n",
    "def dTrainLoss(w, x, y):\n",
    "    grad = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        grad += dLogisticLoss(w, x[i], y[i])\n",
    "    return grad / (x.shape[0] * 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y  = y\n",
    "        \n",
    "        self.w1 = np.random.uniform(2, 2)\n",
    "        self.b1 = np.random.uniform(1, 2)\n",
    "        self.w2 = np.random.uniform(2, 2)\n",
    "        self.b2 = np.random.uniform(1, 2)\n",
    "        \n",
    "        self.lr = 0.01\n",
    "    \n",
    "    def forward_prop(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.X, self.w1) + self.b1)\n",
    "        self.pred = sigmoid(np.dot(self.layer1, self.w2) + self.b2)\n",
    "    \n",
    "        \n",
    "    def backward_prop(self):     \n",
    "        print(\"print\")\n",
    "        \n",
    "        \n",
    "    def fit(self, num_epochs=300):\n",
    "        \n",
    "        cost_val = []\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "            j =  cross_entropy(self.pred, self.y)\n",
    "            cost_val.append(j)\n",
    "            \n",
    "            self.forward_prop()\n",
    "            self.backward_prop()\n",
    "        return cost_val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84261401 0.84261401]\n",
      " [0.84261401 0.97534491]\n",
      " [0.97534491 0.84261401]\n",
      " [0.97534491 0.97534491]]\n",
      "\n",
      " [[0.94359906 0.94359906]\n",
      " [0.94359906 0.95617252]\n",
      " [0.95617252 0.94359906]\n",
      " [0.95617252 0.95617252]]\n"
     ]
    }
   ],
   "source": [
    "nn = NN(inputs, target)\n",
    "nn.forward_prop()\n",
    "print(nn.layer1)\n",
    "print(\"\\n\", nn.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c9180742124a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredicted_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint_one_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {} Loss {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted_output' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training algorithm\n",
    "for epoch in range(epochs):\n",
    "    # Forward Propagation\n",
    "\n",
    "    # Loss\n",
    "    # loss = 0.5 * (target - predicted_output) ** 2\n",
    "    loss = loss.sum()\n",
    "    print_one_line(\"Epoch {} Loss {:.4f}\".format(epoch, loss))\n",
    "\n",
    "    # Backpropagation\n",
    "\n",
    "    # Updating Weights and Biases\n",
    "\n",
    "print('')\n",
    "print(\"Final hidden weights: \", end='')\n",
    "print(*hidden_weights)\n",
    "print(\"Final hidden bias: \", end='')\n",
    "print(*hidden_bias)\n",
    "print(\"Final output weights: \", end='')\n",
    "print(*output_weights)\n",
    "print(\"Final output bias: \", end='')\n",
    "print(*output_bias)\n",
    "\n",
    "print(\"\\nOutput from neural network after 10,000 epochs: \", end='')\n",
    "#print(*predicted_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
