{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeVvci7g7Hf5"
   },
   "source": [
    "# Large scale text analysis with deep learning (3 points)\n",
    "\n",
    "Today we're gonna apply the newly learned tools for the task of predicting job salary.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/3342/media/salary%20prediction%20engine%20v2.png\" width=400px>\n",
    "\n",
    "_Special thanks to [Oleg Vasilev](https://github.com/Omrigan/) for the core assignment idea._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/iris/paulshab/NLP_env/nlp_environ/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1ICDIfkt7Hf7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTCslFnT7Hf8"
   },
   "source": [
    "### About the challenge\n",
    "For starters, let's download and unpack the data from [here]. \n",
    "\n",
    "You can also get it from [yadisk url](https://yadi.sk/d/vVEOWPFY3NruT7) the competition [page](https://www.kaggle.com/c/job-salary-prediction/data) (pick `Train_rev1.*`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "F3sHmaat7Hf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244768, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!wget https://ysda-seminars.s3.eu-central-1.amazonaws.com/Train_rev1.zip\n",
    "#!unzip Train_rev1.zip\n",
    "data = pd.read_csv(\"./Train_rev1.csv\", index_col=None)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fQMHpIc27Hf9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12612628</td>\n",
       "      <td>Engineering Systems Analyst</td>\n",
       "      <td>Engineering Systems Analyst Dorking Surrey Sal...</td>\n",
       "      <td>Dorking, Surrey, Surrey</td>\n",
       "      <td>Dorking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12612830</td>\n",
       "      <td>Stress Engineer Glasgow</td>\n",
       "      <td>Stress Engineer Glasgow Salary **** to **** We...</td>\n",
       "      <td>Glasgow, Scotland, Scotland</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 35000/annum 25-35K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12612844</td>\n",
       "      <td>Modelling and simulation analyst</td>\n",
       "      <td>Mathematical Modeller / Simulation Analyst / O...</td>\n",
       "      <td>Hampshire, South East, South East</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 40000/annum 20-40K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12613049</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 30000/annum 25K-30K negotiable</td>\n",
       "      <td>27500</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12613647</td>\n",
       "      <td>Pioneer, Miser Engineering Systems Analyst</td>\n",
       "      <td>Pioneer, Miser  Engineering Systems Analyst Do...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title  \\\n",
       "0  12612628                        Engineering Systems Analyst   \n",
       "1  12612830                            Stress Engineer Glasgow   \n",
       "2  12612844                   Modelling and simulation analyst   \n",
       "3  12613049  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  12613647         Pioneer, Miser Engineering Systems Analyst   \n",
       "\n",
       "                                     FullDescription  \\\n",
       "0  Engineering Systems Analyst Dorking Surrey Sal...   \n",
       "1  Stress Engineer Glasgow Salary **** to **** We...   \n",
       "2  Mathematical Modeller / Simulation Analyst / O...   \n",
       "3  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  Pioneer, Miser  Engineering Systems Analyst Do...   \n",
       "\n",
       "                         LocationRaw LocationNormalized ContractType  \\\n",
       "0            Dorking, Surrey, Surrey            Dorking          NaN   \n",
       "1        Glasgow, Scotland, Scotland            Glasgow          NaN   \n",
       "2  Hampshire, South East, South East          Hampshire          NaN   \n",
       "3     Surrey, South East, South East             Surrey          NaN   \n",
       "4     Surrey, South East, South East             Surrey          NaN   \n",
       "\n",
       "  ContractTime                       Company          Category  \\\n",
       "0    permanent  Gregory Martin International  Engineering Jobs   \n",
       "1    permanent  Gregory Martin International  Engineering Jobs   \n",
       "2    permanent  Gregory Martin International  Engineering Jobs   \n",
       "3    permanent  Gregory Martin International  Engineering Jobs   \n",
       "4    permanent  Gregory Martin International  Engineering Jobs   \n",
       "\n",
       "                                SalaryRaw  SalaryNormalized        SourceName  \n",
       "0              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  \n",
       "1              25000 - 35000/annum 25-35K             30000  cv-library.co.uk  \n",
       "2              20000 - 40000/annum 20-40K             30000  cv-library.co.uk  \n",
       "3  25000 - 30000/annum 25K-30K negotiable             27500  cv-library.co.uk  \n",
       "4              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jtt1yzGo7Hf9"
   },
   "source": [
    "One problem with salary prediction is that it's oddly distributed: there are many people who are paid standard salaries and a few that get tons o money. The distribution is fat-tailed on the right side, which is inconvenient for MSE minimization.\n",
    "\n",
    "There are several techniques to combat this: using a different loss function, predicting log-target instead of raw target or even replacing targets with their percentiles among all salaries in the training set. We gonna use logarithm for now.\n",
    "\n",
    "_You can read more [in the official description](https://www.kaggle.com/c/job-salary-prediction#description)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BIKHnpQN7Hf-"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')\n",
    "\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data[\"SalaryNormalized\"], bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(data['Log1pSalary'], bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en-_DVlP7Hf-"
   },
   "source": [
    "Our task is to predict one number, __Log1pSalary__.\n",
    "\n",
    "To do so, our model can access a number of features:\n",
    "* Free text: __`Title`__ and  __`FullDescription`__\n",
    "* Categorical: __`Category`__, __`Company`__, __`LocationNormalized`__, __`ContractType`__, and __`ContractTime`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yNe-46YX7Hf-"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "      <th>Log1pSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8879</th>\n",
       "      <td>65444731</td>\n",
       "      <td>CNC Millers &amp; Turners Needed</td>\n",
       "      <td>CNC Millers CNC Turner Needed  Neg per hour  M...</td>\n",
       "      <td>Milton Keynes Buckinghamshire South East</td>\n",
       "      <td>Milton Keynes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Alecto Recruitment</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>22000 - 35000 per annum</td>\n",
       "      <td>28500</td>\n",
       "      <td>totaljobs.com</td>\n",
       "      <td>10.257694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177656</th>\n",
       "      <td>71552758</td>\n",
       "      <td>Second in Department  Maths</td>\n",
       "      <td>Second in Faculty  Mathematics MPS/UPS  TLR **...</td>\n",
       "      <td>Barnsley</td>\n",
       "      <td>Barnsley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hays   Sheffield</td>\n",
       "      <td>Teaching Jobs</td>\n",
       "      <td>21500.00 - 46500.00 per annum + TLR 1c</td>\n",
       "      <td>34000</td>\n",
       "      <td>MyUkJobs</td>\n",
       "      <td>10.434145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131728</th>\n",
       "      <td>70205178</td>\n",
       "      <td>Sales Advisor</td>\n",
       "      <td>Job Reference: **** Hours of work: 40 Days of ...</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>full_time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>British Sky Broadcasting Ltd</td>\n",
       "      <td>Sales Jobs</td>\n",
       "      <td>28,000.00 per year</td>\n",
       "      <td>28000</td>\n",
       "      <td>Jobcentre Plus</td>\n",
       "      <td>10.239996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id                         Title  \\\n",
       "8879    65444731  CNC Millers & Turners Needed   \n",
       "177656  71552758   Second in Department  Maths   \n",
       "131728  70205178                 Sales Advisor   \n",
       "\n",
       "                                          FullDescription  \\\n",
       "8879    CNC Millers CNC Turner Needed  Neg per hour  M...   \n",
       "177656  Second in Faculty  Mathematics MPS/UPS  TLR **...   \n",
       "131728  Job Reference: **** Hours of work: 40 Days of ...   \n",
       "\n",
       "                                     LocationRaw LocationNormalized  \\\n",
       "8879    Milton Keynes Buckinghamshire South East      Milton Keynes   \n",
       "177656                                  Barnsley           Barnsley   \n",
       "131728                                 Cambridge          Cambridge   \n",
       "\n",
       "       ContractType ContractTime                       Company  \\\n",
       "8879            NaN    permanent            Alecto Recruitment   \n",
       "177656          NaN          NaN              Hays   Sheffield   \n",
       "131728    full_time          NaN  British Sky Broadcasting Ltd   \n",
       "\n",
       "                Category                               SalaryRaw  \\\n",
       "8879    Engineering Jobs                 22000 - 35000 per annum   \n",
       "177656     Teaching Jobs  21500.00 - 46500.00 per annum + TLR 1c   \n",
       "131728        Sales Jobs                      28,000.00 per year   \n",
       "\n",
       "        SalaryNormalized      SourceName  Log1pSalary  \n",
       "8879               28500   totaljobs.com    10.257694  \n",
       "177656             34000        MyUkJobs    10.434145  \n",
       "131728             28000  Jobcentre Plus    10.239996  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_columns = [\"Title\", \"FullDescription\"]\n",
    "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
    "TARGET_COLUMN = \"Log1pSalary\"\n",
    "\n",
    "data[categorical_columns] = data[categorical_columns].fillna('NaN') # cast missing values to string \"NaN\"\n",
    "\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlLD58gG7Hf_"
   },
   "source": [
    "### Preprocessing text data\n",
    "\n",
    "Just like last week, applying NLP to a problem begins from tokenization: splitting raw text into sequences of tokens (words, punctuation, etc).\n",
    "\n",
    "__Your task__ is to lowercase and tokenize all texts under `Title` and `FullDescription` columns. Store the tokenized data as a __space-separated__ string of tokens for performance reasons.\n",
    "\n",
    "It's okay to use nltk tokenizers. Assertions were designed for WordPunctTokenizer, slight deviations are okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nWtuksb47Hf_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "2         Mathematical Modeller / Simulation Analyst / O...\n",
      "100002    A successful and high achieving specialist sch...\n",
      "200002    Web Designer  HTML, CSS, JavaScript, Photoshop...\n",
      "Name: FullDescription, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw text:\")\n",
    "print(data[\"FullDescription\"][2::100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_to_lower(Series):\n",
    "    new_series = []\n",
    "    \n",
    "    for row in Series:\n",
    "        row = str(row)\n",
    "        row = row.lower()\n",
    "        my_string = ' '.join(tokenizer.tokenize(row))\n",
    "        new_series.append(my_string)\n",
    "    return new_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rCRkS-ig7Hf_"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#TODO YOUR CODE HERE\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "data[\"FullDescription\"] = tokenize_and_to_lower(data[\"FullDescription\"])\n",
    "data[\"Title\"] = tokenize_and_to_lower(data[\"Title\"])\n",
    "\n",
    "#tokenize_and_to_lower(data[\"FullDescription\"][2::100000])\n",
    "#new_series = tokenize_and_to_lower(data[\"FullDescription\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUIUaBBr7HgA"
   },
   "source": [
    "Now we can assume that our text is a space-separated list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PlyxwnPI7HgA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized:\n",
      "2         mathematical modeller / simulation analyst / o...\n",
      "100002    a successful and high achieving specialist sch...\n",
      "200002    web designer html , css , javascript , photosh...\n",
      "Name: FullDescription, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized:\")\n",
    "print(data[\"FullDescription\"][2::100000])\n",
    "assert data[\"FullDescription\"][2][:50] == 'mathematical modeller / simulation analyst / opera'\n",
    "assert data[\"Title\"][54321] == 'international digital account manager ( german )'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nITtlGkb7HgA"
   },
   "source": [
    "Not all words are equally useful. Some of them are typos or rare words that are only present a few times. \n",
    "\n",
    "Let's count how many times is each word present in the data so that we can build a \"white list\" of known words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dict(x):\n",
    "    values = {}\n",
    "    for row in tqdm.tqdm(x):\n",
    "        vals = row.split()\n",
    "        for val in vals:\n",
    "            if val not in values:\n",
    "                values[val] = 1\n",
    "            else:\n",
    "                values[val] += 1\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JsEA5jb07HgA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 244768/244768 [00:12<00:00, 19020.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import tqdm\n",
    "\n",
    "# Count how many times does each token occur in both \"Title\" and \"FullDescription\" in total\n",
    "\n",
    "token_counts = count_dict(data[\"FullDescription\"] + data[\"Title\"])\n",
    "token_counts = Counter(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253743"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aRHGqFH97HgB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens : 253743\n",
      "('and', 2657309)\n",
      "('.', 2453599)\n",
      "(',', 2317992)\n",
      "('the', 2080902)\n",
      "('to', 2018611)\n",
      "...\n",
      "('stephanietraveltraderecruitmnt', 1)\n",
      "('ruabon', 1)\n",
      "('lowehays', 1)\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "print(\"Total unique tokens :\", len(token_counts))\n",
    "print('\\n'.join(map(str, token_counts.most_common(n=5))))\n",
    "print('...')\n",
    "print('\\n'.join(map(str, token_counts.most_common()[-3:])))\n",
    "\n",
    "assert token_counts.most_common(1)[0][1] in  range(2600000, 2700000)\n",
    "#assert len(token_counts) in range(200000, 210000)\n",
    "print('Correct!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YGF2PnDz7HgB"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEGCAYAAACevtWaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAASHElEQVR4nO3de7BdZ1nH8e+P1LYC2gvNMLUXkpJOIcIMlzPlpk7lmkJDHURpYORiJQJTFHFG0sE/4A9nQPACQ4cSSy2gthRksJcwFZFahhZsyq3pJRBKsalAUqoBGQUCj3+sFbo5nJPsZO+dffZ7vp+ZM1nrXWuv9b57nTzn3c9697tSVUiS2vKgaVdAkjR+BndJapDBXZIaZHCXpAYZ3CWpQUdMuwIAJ5xwQq1atWra1ZCkmXLLLbfcV1UrF9o21eCeZD2wfs2aNWzdunWaVZGkmZPk64ttm2papqqurqqNxxxzzDSrIUnNMecuSQ0yuEtSgwzuktSgqQb3JOuTbN6zZ880qyFJzfGGqiQ1yLSMJDXI4C5JDVoS31AdxapN1y5YfvdbnneYayJJS4c9d0lqkMFdkhpkcJekBjnOXZIa5Dh3SWqQaRlJapDBXZIaZHCXpAYZ3CWpQQZ3SWqQwV2SGmRwl6QGGdwlqUEGd0lq0NiDe5KzknwqycVJzhr38SVJBzZUcE9yaZJdSbbNK1+XZHuSHUk29cUF/A9wNLBzvNWVJA1j2J77ZcC6wYIkK4CLgLOBtcCGJGuBT1XV2cAbgDePr6qSpGENFdyr6gbg/nnFZwI7ququqvoBcAVwblX9uN/+X8BRix0zycYkW5Ns3b179yFUXZK0mFFy7icB9wys7wROSvKCJO8BPgC8a7EXV9XmqpqrqrmVK1eOUA1J0nxjf4ZqVX0E+Mgw+yZZD6xfs2bNuKshScvaKD33e4FTBtZP7suG5nzukjQZowT3m4HTk6xOciRwHnDVwRzAJzFJ0mQMOxTycuAm4IwkO5OcX1V7gQuA64A7gCur6raDObk9d0majKFy7lW1YZHyLcCWsdZIkjQyH5AtSQ3yAdmS1CAnDpOkBpmWkaQGmZaRpAaZlpGkBpmWkaQGmZaRpAaZlpGkBhncJalBBndJapA3VCWpQd5QlaQGmZaRpAYZ3CWpQQZ3SWqQN1QlqUHeUJWkBpmWkaQGGdwlqUEGd0lqkMFdkhpkcJekBhncJalBjnOXpAY5zl2SGmRaRpIaZHCXpAYZ3CWpQQZ3SWqQwV2SGmRwl6QGGdwlqUEGd0lqkMFdkho0keCe5CFJtiY5ZxLHlyTt31DBPcmlSXYl2TavfF2S7Ul2JNk0sOkNwJXjrKgkaXjD9twvA9YNFiRZAVwEnA2sBTYkWZvkWcDtwK4x1lOSdBCOGGanqrohyap5xWcCO6rqLoAkVwDnAg8FHkIX8P83yZaq+vH8YybZCGwEOPXUUw+5AZKknzVUcF/EScA9A+s7gSdV1QUASV4O3LdQYAeoqs3AZoC5ubkaoR6SpHlGCe77VVWXHWifJOuB9WvWrJlUNSRpWRpltMy9wCkD6yf3ZUNzPndJmoxRgvvNwOlJVic5EjgPuOpgDuCTmCRpMoYdCnk5cBNwRpKdSc6vqr3ABcB1wB3AlVV128Gc3J67JE3GsKNlNixSvgXYMtYaSZJG5gOyJalBPiBbkhrkxGGS1CDTMpLUINMyktQg0zKS1CDTMpLUINMyktQg0zKS1CCDuyQ1yOAuSQ3yhqokNcgbqpLUINMyktQgg7skNcjgLkkN8oaqJDXIG6qS1CDTMpLUIIO7JDXI4C5JDTpi2hWYlFWbrl2w/O63PO8w10SSDj977pLUIIO7JDXIce6S1CDHuUtSg0zLSFKDDO6S1CCDuyQ1yOAuSQ0yuEtSgwzuktQgg7skNcjgLkkNanbisMUsNqEYOKmYpHaMveee5NFJLk7y4SSvHvfxJUkHNlRwT3Jpkl1Jts0rX5dke5IdSTYBVNUdVfUq4LeBp42/ypKkAxm2534ZsG6wIMkK4CLgbGAtsCHJ2n7b84FrgS1jq6kkaWhDBfequgG4f17xmcCOqrqrqn4AXAGc2+9/VVWdDbxksWMm2Zhka5Ktu3fvPrTaS5IWNMoN1ZOAewbWdwJPSnIW8ALgKPbTc6+qzcBmgLm5uRqhHpKkecY+WqaqrgeuH2bfJOuB9WvWrBl3NSRpWRtltMy9wCkD6yf3ZUNzPndJmoxRgvvNwOlJVic5EjgPuOpgDuCTmCRpMoYdCnk5cBNwRpKdSc6vqr3ABcB1wB3AlVV128Gc3J67JE3GUDn3qtqwSPkWHO4oSUvOVKcfWGo3VBebmsBpCSTNGh+QLUkNclZISWrQVIO7o2UkaTJMy0hSg0zLSFKDHC0zBEfRSJo1pmUkqUGmZSSpQQZ3SWqQwV2SGuQ4d0lqkDdUJalBUx0KOescIilpqTLnLkkNMrhLUoO8oSpJDfKGqiQ1yLSMJDXI0TIT4CgaSdNmz12SGmRwl6QGmZY5jEzXSDpc7LlLUoMc5y5JDXKcuyQ1yJz7EmAuXtK4mXOXpAbZc1/C7NFLOlT23CWpQQZ3SWqQaZkZZLpG0oHYc5ekBtlzb4g9ekn72HOXpAZNpOee5DeA5wG/CLy3qv55EufRcOzRS8vP0D33JJcm2ZVk27zydUm2J9mRZBNAVX20ql4JvAp40XirLEk6kIPpuV8GvAt4/76CJCuAi4BnATuBm5NcVVW397v8ab9dS5A9eqldQ/fcq+oG4P55xWcCO6rqrqr6AXAFcG46bwU+VlWfW+h4STYm2Zpk6+7duw+1/pKkBYx6Q/Uk4J6B9Z192WuBZwIvTPKqhV5YVZuraq6q5lauXDliNSRJgyZyQ7Wq3gm880D7JVkPrF+zZs0kqqFDtFi6BkzZSLNi1J77vcApA+sn92VDcT53SZqMUXvuNwOnJ1lNF9TPA1487Ivtuc+e/fXqF2JPX5qOgxkKeTlwE3BGkp1Jzq+qvcAFwHXAHcCVVXXbsMe05y5JkzF0z72qNixSvgXYMrYaSZJGNtW5ZUzLLF+OsZcma6rBvaquBq6em5t75TTroaXPPwbSwXHiMElqkGkZTdTBjq6RNB6mZbSk+MdAGg8f1qGZZi5eWthUc+5J1ifZvGfPnmlWQ5KaY1pG4vB8AvBThg4n0zJqkoFUy53BXdoPZ8jUrDK4S2PmiB8tBd5QlaQGeUNVOkT20LWUOf2AJDXInLs0ZeMa2eMIIQ0yuGtZmaVUisFao3DiMKlx/pFYnryhKs2YWfr0oekxLSNpyfBTxvg4WkaSGmTPXdKSZ4/+4Nlzl6QGGdwlqUGmZST9lIMdjbO/1IjplOlxnLu0TE1zSKXDOSfPce6SRrIUA7WfGMy5S1KTDO6S1CCDuyQ1yNEykmbWuPL9LeboDe6StIhZDvoGd0nLxnLq6Ztzl6QGGdwlqUEGd0lq0NiDe5LTkrw3yYfHfWxJ0nCGuqGa5FLgHGBXVT1moHwd8A5gBXBJVb2lqu4Czje4S9IDDvdN2GF77pcB6wYLkqwALgLOBtYCG5KsHWvtJEmHZKiee1XdkGTVvOIzgR19T50kVwDnArcPc8wkG4GNAKeeeuqw9ZWkJWspTaI2Ss79JOCegfWdwElJHpbkYuDxSS5c7MVVtbmq5qpqbuXKlSNUQ5I039i/xFRV3wZeNcy+zucuSZMxSs/9XuCUgfWT+7KhVdXVVbXxmGOOGaEakqT5RgnuNwOnJ1md5EjgPOCqgzlAkvVJNu/Zs2eEakiS5hsquCe5HLgJOCPJziTnV9Ve4ALgOuAO4Mqquu1gTm7PXZImY9jRMhsWKd8CbBlrjSRJI5vq9AOmZSRpMqYa3E3LSNJkOHGYJDUoVTXtOpBkN/D1Q3z5CcB9Y6zOLLDNy4NtXh5GafMjqmrBb4EuieA+iiRbq2pu2vU4nGzz8mCbl4dJtdm0jCQ1yOAuSQ1qIbhvnnYFpsA2Lw+2eXmYSJtnPucuSfpZLfTcJUnzGNwlqUEzHdyTrEuyPcmOJJumXZ9DleSUJJ9McnuS25L8YV9+fJKPJ/lK/+9xfXmSvLNv95eSPGHgWC/r9/9KkpdNq03DSrIiyeeTXNOvr07y2b5tH+xnHCXJUf36jn77qoFjXNiXb0/ynCk1ZShJjk3y4SR3JrkjyVNav85J/qj/vd6W5PIkR7d2nZNcmmRXkm0DZWO7rkmemOTW/jXvTJIDVqqqZvKH7qHcXwVOA44EvgisnXa9DrEtJwJP6Jd/Afgy3XNp/xzY1JdvAt7aLz8X+BgQ4MnAZ/vy44G7+n+P65ePm3b7DtD21wP/AFzTr18JnNcvXwy8ul9+DXBxv3we8MF+eW1/7Y8CVve/Eyum3a79tPd9wO/1y0cCx7Z8neme2PY14OcHru/LW7vOwK8BTwC2DZSN7boC/97vm/61Zx+wTtN+U0Z4M58CXDewfiFw4bTrNaa2/RPwLGA7cGJfdiKwvV9+D7BhYP/t/fYNwHsGyn9qv6X2Q/eAl08ATweu6X9x7wOOmH+N6aaWfkq/fES/X+Zf98H9ltoPcEwf6DKvvNnrzAOP4zy+v27XAM9p8ToDq+YF97Fc137bnQPlP7XfYj+znJZZ8BmuU6rL2PQfQx8PfBZ4eFV9o9/0TeDh/fJibZ+19+SvgT8BftyvPwz47+qeFQA/Xf+ftK3fvqfff5bavBrYDfxtn4q6JMlDaPg6V9W9wNuB/wC+QXfdbqHt67zPuK7rSf3y/PL9muXg3pwkDwX+EXhdVX1ncFt1f7KbGbea5BxgV1XdMu26HEZH0H10f3dVPR74Ht3H9Z9o8DofB5xL94ftl4CHAOumWqkpmMZ1neXgPvIzXJeSJD9HF9j/vqo+0hd/K8mJ/fYTgV19+WJtn6X35GnA85PcDVxBl5p5B3Bskn0PkRms/0/a1m8/Bvg2s9XmncDOqvpsv/5humDf8nV+JvC1qtpdVT8EPkJ37Vu+zvuM67re2y/PL9+vWQ7uIz/Ddano73y/F7ijqv5yYNNVwL475i+jy8XvK39pf9f9ycCe/uPfdcCzkxzX95ie3ZctOVV1YVWdXFWr6K7dv1bVS4BPAi/sd5vf5n3vxQv7/asvP68fZbEaOJ3u5tOSU1XfBO5JckZf9Azgdhq+znTpmCcneXD/e76vzc1e5wFjua79tu8keXL/Hr504FiLm/ZNiBFvYDyXbmTJV4E3Trs+I7TjV+g+sn0J+EL/81y6XOMngK8A/wIc3+8f4KK+3bcCcwPH+l1gR//zimm3bcj2n8UDo2VOo/tPuwP4EHBUX350v76j337awOvf2L8X2xliFMGU2/o4YGt/rT9KNyqi6esMvBm4E9gGfIBuxEtT1xm4nO6ewg/pPqGdP87rCsz1799XgXcx76b8Qj9OPyBJDZrltIwkaREGd0lqkMFdkhpkcJekBhncJalBBncteUn+KsnrBtavS3LJwPpfJHn9IR77rPQzUh5O6WaHfM3hPq+WD4O7ZsGngacCJHkQcALwywPbnwrcOMyBkqwYe+0OzbF0MyBKE2Fw1yy4kW7mQOiC+jbgu/03+Y4CHg18Lskz+gm5bu3n1z4KIMndSd6a5HPAb6V7DsCd/foLFjphunnm355uDvIvJXltX76/c5zQL88lub5fflO/3/VJ7kryB/0p3gI8MskXkrwtyYlJbujXtyX51Qm8j1pGjjjwLtJ0VdV/Jtmb5FS6XvpNdLPiPYVu1sBb6ToqlwHPqKovJ3k/8Gq6mScBvl1VT0hyNN03Bp9O9y3ADy5y2o10U7g+rqr2pnvwwtEHOMdiHgX8Ot1c/duTvJtuwrDHVNXjAJL8Md1Xzf+s/3Tx4CHfHmlB9tw1K26kC+z7gvtNA+ufBs6gm6Dqy/3+76N7gMI++4L4o/r9vlLd17P/bpHzPZNuzuy9AFV1/xDnWMy1VfX9qrqPbvKohy+wz83AK5K8CXhsVX13iONKizK4a1bsy7s/li4t8xm6nvuw+fbvTa5qAOzlgf9PR8/b9v2B5R+xwCfmqrqB7g/FvcBlSV46iUpq+TC4a1bcCJwD3F9VP+p70sfSBfgb6SaTWpVkTb//7wD/tsBx7uz3e2S/vmGR830c+P1909ImOf4A57gbeGK//JtDtOe7dGka+uM/AvhWVf0NcAndVMDSITO4a1bcSjdK5jPzyvZU1X1V9X/AK4APJbmV7ulOF88/SL/fRuDa/obqrvn79C6hm672S0m+CLz4AOd4M/COJFvpeuf7VVXfBj7d3zx9G93MmF9M8nngRXRz20uHzFkhJalB9twlqUEGd0lqkMFdkhpkcJekBhncJalBBndJapDBXZIa9P92YhLEqd5EpQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see how many words are there for each count\n",
    "plt.hist(list(token_counts.values()), range=[0, 10**4], bins=50, log=True)\n",
    "plt.xlabel(\"Word counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbavdoVB7HgB"
   },
   "source": [
    "__Task 1.1__ Get a list of all tokens that occur at least 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QRFvn56P7HgC"
   },
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "\n",
    "# tokens from token_counts keys that had at least min_count occurrences throughout the dataset\n",
    "tokens = sorted(t for t, c in token_counts.items() if c >= min_count)#TODO<YOUR CODE HERE>\n",
    "\n",
    "# Add a special tokens for unknown and empty words\n",
    "UNK, PAD = \"UNK\", \"PAD\"\n",
    "tokens = [UNK, PAD] + tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36720"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3sl4-9AI7HgC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36720\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", len(tokens))\n",
    "assert type(tokens) == list\n",
    "#assert len(tokens) in range(32000, 35000)\n",
    "assert 'me' in tokens\n",
    "assert UNK in tokens\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqQxgNg97HgC"
   },
   "source": [
    "__Task 1.2__ Build an inverse token index: a dictionary from token(string) to it's index in `tokens` (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MHgQsZkd7HgD"
   },
   "outputs": [],
   "source": [
    "token_to_id = {t: i for i, t in enumerate(tokens)}\n",
    "id_to_token = {i: t for i, t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pCd0n61R7HgD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(token_to_id, dict)\n",
    "assert len(token_to_id) == len(tokens)\n",
    "for tok in tokens:\n",
    "    assert tokens[token_to_id[tok]] == tok\n",
    "\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pAx-IRb7HgD"
   },
   "source": [
    "And finally, let's use the vocabulary you've built to map text lines into neural network-digestible matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eUS-t4wi7HgD"
   },
   "outputs": [],
   "source": [
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, max_len=200):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "    \n",
    "    #print(sequences)\n",
    "    #print(max(list(map(len, sequences)), max_len))\n",
    "    #print(max(map(len, sequences)), max_len or float('inf'))\n",
    "    \n",
    "    \n",
    "    #max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    #max_len = max(max(map(len, sequences)), max_len)\n",
    "    \n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"hello how are . . . \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DtbiylrF7HgD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines:\n",
      "engineering systems analyst\n",
      "hr assistant\n",
      "senior ec & i engineer\n",
      "\n",
      "Matrix:\n",
      "[[12827 32378  3863     1     1     1     1     1     1     1]\n",
      " [17049  4570     1     1     1     1     1     1     1     1]\n",
      " [29853 12186    16 17243 12821     1     1     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lines:\")\n",
    "print('\\n'.join(data[\"Title\"][::100000].values), end='\\n\\n')\n",
    "print(\"Matrix:\")\n",
    "print(as_matrix(data[\"Title\"][::100000], max_len=10))\n",
    "#print(as_matrix(data[\"Title\"][::100000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLKrL13Z7HgE"
   },
   "source": [
    "Now let's  encode the categirical data we have.\n",
    "\n",
    "As usual, we shall use one-hot encoding for simplicity. Kudos if you implement more advanced encodings: tf-idf, pseudo-time-series, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3f6fuIWj7HgE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float32'>, sparse=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# we only consider top-1k most frequent companies to minimize memory usage\n",
    "top_companies, top_counts = zip(*Counter(data['Company']).most_common(1000))\n",
    "recognized_companies = set(top_companies)\n",
    "data[\"Company\"] = data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
    "\n",
    "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYBJPjlQ7HgF"
   },
   "source": [
    "### The deep learning part\n",
    "\n",
    "Once we've learned to tokenize the data, let's design a machine learning experiment.\n",
    "\n",
    "As before, we won't focus too much on validation, opting for a simple train-test split.\n",
    "\n",
    "__To be completely rigorous,__ we've comitted a small crime here: we used the whole data for tokenization and vocabulary building. A more strict way would be to do that part on training set only. You may want to do that and measure the magnitude of changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vCKfRvm47HgF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size =  195814\n",
      "Validation size =  48954\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "data_train.index = range(len(data_train))\n",
    "data_val.index = range(len(data_val))\n",
    "\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ugR3wleU7HgF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def to_tensors(batch, device):\n",
    "    batch_tensors = dict()\n",
    "    for key, arr in batch.items():\n",
    "        if key in [\"FullDescription\", \"Title\"]:\n",
    "            batch_tensors[key] = torch.tensor(arr, device=device, dtype=torch.int64)\n",
    "        else:\n",
    "            batch_tensors[key] = torch.tensor(arr, device=device)\n",
    "    return batch_tensors\n",
    "\n",
    "def make_batch(data, max_len_title=20, max_len_desc=80, word_dropout=0, device=torch.device('cuda:0')):\n",
    "    \"\"\"\n",
    "    Creates a keras-friendly dict from the batch data.\n",
    "    :param word_dropout: replaces token index with UNK_IX with this probability\n",
    "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
    "    \"\"\"\n",
    "    batch = {}\n",
    "    batch[\"Title\"] = as_matrix(data[\"Title\"].values, max_len_title)\n",
    "    batch[\"FullDescription\"] = as_matrix(data[\"FullDescription\"].values, max_len_desc)\n",
    "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
    "    \n",
    "    if word_dropout != 0:\n",
    "        batch[\"FullDescription\"] = apply_word_dropout(batch[\"FullDescription\"], 1. - word_dropout)\n",
    "    \n",
    "    if TARGET_COLUMN in data.columns:\n",
    "        batch[TARGET_COLUMN] = data[TARGET_COLUMN].values\n",
    "    \n",
    "    return to_tensors(batch, device)\n",
    "\n",
    "def apply_word_dropout(matrix, keep_prop, replace_with=UNK_IX, pad_ix=PAD_IX,):\n",
    "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prop, 1 - keep_prop])\n",
    "    dropout_mask &= matrix != pad_ix\n",
    "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rWOqqPrZ7HgG",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " torch.Size([5, 20])\n",
      "\n",
      " torch.Size([5, 80])\n",
      "\n",
      " torch.Size([5, 3768])\n",
      "\n",
      " torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "#batch = make_batch(data_train[:5], max_len=10)\n",
    "batch = make_batch(data_train[:5], max_len_title=20, max_len_desc=80)\n",
    "print(\"\\n\", batch[\"Title\"].shape)\n",
    "print(\"\\n\", batch[\"FullDescription\"].shape)\n",
    "print(\"\\n\", batch[\"Categorical\"].shape)\n",
    "print(\"\\n\", batch[TARGET_COLUMN].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bzAPMi47HgH"
   },
   "source": [
    "#### Architecture\n",
    "\n",
    "Our basic model consists of three branches:\n",
    "* Title encoder\n",
    "* Description encoder\n",
    "* Categorical features encoder\n",
    "\n",
    "We will then feed all 3 branches into one common network that predicts salary.\n",
    "\n",
    "![scheme](https://github.com/yandexdataschool/nlp_course/raw/master/resources/w2_conv_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8VUKrHt7HgI"
   },
   "source": [
    "This clearly doesn't fit into keras' __Sequential__ interface. To build such a network, one will have to use PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eFeUfVqd7HgJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3768"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "len(categorical_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "MLd4Molr7HgJ"
   },
   "outputs": [],
   "source": [
    "class SalaryPredictor(nn.Module):\n",
    "    def __init__(self, max_title=None, max_desc=None,  num_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_), hid_size=64, embedding_size=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.title_emb = nn.Embedding(num_tokens, embedding_size)     \n",
    "        self.desc_emb = nn.Embedding(num_tokens, embedding_size)  \n",
    "        self.predict = nn.Linear(48, 1)\n",
    "        \n",
    "        self.cnn_title = nn.Sequential(\n",
    "            #nn.Conv2d(1, 3, kernel_size=5, padding=2),            # ( in_channels, out_channels, window_size, )\n",
    "            nn.Conv1d(max_title, 3, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # kernel size, stride\n",
    "        )\n",
    "        \n",
    "        self.cnn_dec = nn.Sequential(\n",
    "            #nn.Conv2d(1, 3, kernel_size=5, padding=2),             # ( in_channels, out_channels, window_size, )\n",
    "            nn.Conv1d(max_desc, 3, kernel_size=5, padding=2),     \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # kernel size, stride\n",
    "        )\n",
    "        \n",
    "        self.cat_enc = nn.Sequential(\n",
    "            nn.Linear(n_cat_features, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        # YOUR CODE HERE\n",
    "        print(\"title_raw\", batch[\"Title\"].shape)\n",
    "        #title_emb = torch.unsqueeze(self.title_emb(batch[\"Title\"]), 1)\n",
    "        title_emb = self.title_emb(batch[\"Title\"])\n",
    "        print(\"title_emb\", title_emb.shape)\n",
    "        title_tensor = self.cnn_title(title_emb)\n",
    "        print(\"title cnn\", title_tensor.shape)\n",
    "        title_tensor = title_tensor.view(title_tensor.size(0), -1)  # flatten\n",
    "        print(\"title tensor:\", title_tensor.shape)\n",
    "        \n",
    "        #print(\"\\ntitle_raw\", batch[\"FullDescription\"].shape)\n",
    "        #desc_emb = torch.unsqueeze(self.desc_emb(batch[\"FullDescription\"]), 1)\n",
    "        desc_emb = self.desc_emb(batch[\"FullDescription\"])\n",
    "        #print(\"desc_emb\", desc_emb.shape)\n",
    "        desc_tensor = self.cnn_dec(desc_emb)\n",
    "        #print(\"desc_cnn\", desc_tensor.shape)\n",
    "        desc_tensor = desc_tensor.view(desc_tensor.size(0), -1)  # flatten\n",
    "        #print(\"desc_flatten\", desc_tensor.shape)\n",
    "        \n",
    "        cat_enc_tensor = self.cat_enc(batch[\"Categorical\"])\n",
    "        #print(\"\\ncat_tensor: \", cat_enc_tensor.shape)\n",
    "        \n",
    "        X = torch.cat((title_tensor, desc_tensor, cat_enc_tensor), 1)\n",
    "        X = self.predict(X)\n",
    "        \n",
    "        return torch.squeeze(X)\n",
    "        \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36720\n",
      "tensor([[29853, 32112, 36223,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [31462,  1834, 21250, 22121, 17583, 25306,  5798,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [12601, 32628, 19800,    34, 10683, 31381,    66,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [ 9527, 18733, 12821, 20432,  1014,  5705, 10395,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [11879,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(batch['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "l0mzJZNL7HgK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 80])\n",
      "torch.Size([5, 20])\n",
      "title_raw torch.Size([5, 20])\n",
      "title_emb torch.Size([5, 20, 32])\n",
      "title cnn torch.Size([5, 1, 16])\n",
      "title tensor: torch.Size([5, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = make_batch(data_train[:5], max_len_title=20, max_len_desc=80)\n",
    "model = SalaryPredictor(max_title=20, max_desc=80)\n",
    "model.to('cuda:0')\n",
    "\n",
    "print(batch['Title'].shape)\n",
    "print(batch['FullDescription'].shape)\n",
    "\n",
    "\n",
    "dummy_pred = model(batch)\n",
    "dummy_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "id": "LpkKgE8H7HgK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "tensor(103.0352, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#model = SalaryPredictor(max_title=20, max_desc=80)\n",
    "batch = make_batch(data_train[:100], max_len_title=20, max_len_desc=80)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "dummy_pred = model(batch)\n",
    "print(dummy_pred.shape)\n",
    "dummy_loss = criterion(dummy_pred, batch[TARGET_COLUMN])\n",
    "print(dummy_loss)\n",
    "assert dummy_pred.shape == torch.Size([100])\n",
    "assert len(torch.unique(dummy_pred)) > 20, \"model returns suspiciously few unique outputs. Check your initialization\"\n",
    "assert dummy_loss.ndim == 0 and 0. <= dummy_loss <= 250., \"make sure you minimize MSE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNLAs6ie7HgK"
   },
   "source": [
    "#### Training and evaluation\n",
    "\n",
    "As usual, we gonna feed our monster with random minibatches of data. \n",
    "\n",
    "As we train, we want to monitor not only loss function, which is computed in log-space, but also the actual error measured in dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "id": "qTlEr0OD7HgK"
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(data, batch_size=256, shuffle=True, cycle=False, device=torch.device('cuda:0'), **kwargs):\n",
    "    \"\"\" iterates minibatches of data in random order \"\"\"\n",
    "    while True:\n",
    "        indices = np.arange(len(data))\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            batch = make_batch(data.iloc[indices[start : start + batch_size]])\n",
    "            yield batch\n",
    "        \n",
    "        if not cycle: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OK6cOBo7HgL"
   },
   "source": [
    "### Model training\n",
    "\n",
    "We can now fit our model the usual minibatch way. The interesting part is that we train on an infinite stream of minibatches, produced by `iterate_minibatches` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "id": "uwMeF9OD7HgL"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "id": "YAsXUF4T7HgM"
   },
   "outputs": [],
   "source": [
    "def print_metrics(model, data, batch_size=BATCH_SIZE, name=\"\", **kw):\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterate_minibatches(data, batch_size=batch_size, shuffle=False, **kw):\n",
    "            #print(batch['Title'].shape)\n",
    "            batch_pred = model(batch)\n",
    "            #print(batch_pred.shape)\n",
    "            squared_error += torch.sum(torch.square(batch_pred - batch[TARGET_COLUMN]))\n",
    "            abs_error += torch.sum(torch.abs(batch_pred - batch[TARGET_COLUMN]))\n",
    "            num_samples += len(batch)\n",
    "    mse = squared_error.detach().cpu().numpy() / num_samples\n",
    "    mae = abs_error.detach().cpu().numpy() / num_samples\n",
    "    print(\"%s results:\" % (name or \"\"))\n",
    "    print(\"Mean square error: %.5f\" % mse)\n",
    "    print(\"Mean absolute error: %.5f\" % mae)\n",
    "    return mse, mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 150.16211\n",
      "Mean absolute error: 13.68347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150.162109375, 13.683465957641602)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_metrics(model, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "id": "Uht93leD7HgM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iris/paulshab/NLP_env/nlp_environ/lib/python3.7/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae4fe4541a34d4182db8f8083d8c783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.79030\n",
      "Mean absolute error: 1.33434\n",
      "epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6278442b5ef640cab462780185b3ba95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.69371\n",
      "Mean absolute error: 1.27332\n",
      "epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36815f990d9d496085bbe6477dbbb2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.70334\n",
      "Mean absolute error: 1.27423\n",
      "epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac06888e30864b4395f3763a410595c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.65720\n",
      "Mean absolute error: 1.23991\n",
      "epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a009ad5a0f547328fce68342b36c7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.60100\n",
      "Mean absolute error: 1.20279\n"
     ]
    }
   ],
   "source": [
    "model = SalaryPredictor(max_title=20, max_desc=80).to(DEVICE)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(EPOCHS):    # why epochs\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    model.train()\n",
    "    for i, batch in tqdm.tqdm_notebook(enumerate(\n",
    "            iterate_minibatches(data_train, batch_size=BATCH_SIZE, device=DEVICE)),\n",
    "            total=len(data_train) // BATCH_SIZE\n",
    "        ):\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch[TARGET_COLUMN])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print_metrics(model, data_val)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce-6HEDD7HgM"
   },
   "source": [
    "### Bonus part: explaining model predictions\n",
    "\n",
    "It's usually a good idea to understand how your model works before you let it make actual decisions. It's simple for linear models: just see which words learned positive or negative weights. However, its much harder for neural networks that learn complex nonlinear dependencies.\n",
    "\n",
    "There are, however, some ways to look inside the black box:\n",
    "* Seeing how model responds to input perturbations\n",
    "* Finding inputs that maximize/minimize activation of some chosen neurons (_read more [on distill.pub](https://distill.pub/2018/building-blocks/)_)\n",
    "* Building local linear approximations to your neural network: [article](https://arxiv.org/abs/1602.04938), [eli5 library](https://github.com/TeamHG-Memex/eli5/tree/master/eli5/formatters)\n",
    "\n",
    "Today we gonna try the first method just because it's the simplest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "id": "mPtKgD4H7HgM"
   },
   "outputs": [],
   "source": [
    "def explain(model, sample, col_name='Title'):\n",
    "    \"\"\" Computes the effect each word had on model predictions \"\"\"\n",
    "    sample = dict(sample)\n",
    "    sample_col_tokens = [tokens[token_to_id.get(tok, 0)] for tok in sample[col_name].split()]\n",
    "    print(sample_col_tokens)\n",
    "    data_drop_one_token = pd.DataFrame([sample] * (len(sample_col_tokens) + 1))\n",
    "\n",
    "    for drop_i in range(len(sample_col_tokens)):\n",
    "        data_drop_one_token.loc[drop_i, col_name] = ' '.join(UNK if i == drop_i else tok\n",
    "                                                   for i, tok in enumerate(sample_col_tokens)) \n",
    "    \n",
    "    \n",
    "    print(model.predict(make_batch(data_drop_one_token)))\n",
    "    *predictions_drop_one_token, baseline_pred = model.predict(make_batch(data_drop_one_token))[:, 0]\n",
    "    diffs = baseline_pred - predictions_drop_one_token\n",
    "    return list(zip(sample_col_tokens, diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sales', 'specialist', 'iv', 'access', 'and', 'infusion']"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_col_tokens = [tokens[token_to_id.get(tok, 0)] for tok in sample['Title'].split()]\n",
    "sample_col_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dict(data.loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sales', 'specialist', 'iv', 'access', 'and', 'infusion']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10910/527616440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m36605\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens_and_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Title\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_10910/4098762751.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(model, sample, col_name)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_drop_one_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;34m*\u001b[0m\u001b[0mpredictions_drop_one_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_drop_one_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredictions_drop_one_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/paulshab/NLP_env/nlp_environ/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/paulshab/NLP_env/nlp_environ/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/paulshab/NLP_env/nlp_environ/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not dict"
     ]
    }
   ],
   "source": [
    "i = 36605\n",
    "tokens_and_weights = explain(model, data.loc[i], \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "id": "JWJrhijq7HgM"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display_html\n",
    "\n",
    "\n",
    "def draw_html(tokens_and_weights, cmap=plt.get_cmap(\"bwr\"), display=True,\n",
    "              token_template=\"\"\"<span style=\"background-color: {color_hex}\">{token}</span>\"\"\",\n",
    "              font_style=\"font-size:14px;\"\n",
    "             ):\n",
    "    \n",
    "    def get_color_hex(weight):\n",
    "        rgba = cmap(1. / (1 + np.exp(weight)), bytes=True)\n",
    "        return '#%02X%02X%02X' % rgba[:3]\n",
    "    \n",
    "    tokens_html = [\n",
    "        token_template.format(token=token, color_hex=get_color_hex(weight))\n",
    "        for token, weight in tokens_and_weights\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    raw_html = \"\"\"<p style=\"{}\">{}</p>\"\"\".format(font_style, ' '.join(tokens_html))\n",
    "    if display:\n",
    "        display_html(HTML(raw_html))\n",
    "        \n",
    "    return raw_html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "id": "UatSR6PT7HgN"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10910/3557230803.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m36605\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens_and_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Title\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdraw_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_and_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont_style\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'font-size:20px;'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokens_and_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FullDescription\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10910/18338336.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(model, sample, col_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                    for i, tok in enumerate(sample_col_tokens)) \n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;34m*\u001b[0m\u001b[0mpredictions_drop_one_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_drop_one_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredictions_drop_one_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_col_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/paulshab/NLP_env/nlp_environ/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/paulshab/NLP_env/nlp_environ/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/paulshab/NLP_env/nlp_environ/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not dict"
     ]
    }
   ],
   "source": [
    "i = 36605\n",
    "tokens_and_weights = explain(model, data.loc[i], \"Title\")\n",
    "draw_html([(tok, weight * 5) for tok, weight in tokens_and_weights], font_style='font-size:20px;');\n",
    "\n",
    "tokens_and_weights = explain(model, data.loc[i], \"FullDescription\")\n",
    "draw_html([(tok, weight * 10) for tok, weight in tokens_and_weights]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "id": "N3h8O2HK7HgN"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10910/397191623.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12077\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens_and_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Title\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdraw_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_and_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont_style\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'font-size:20px;'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokens_and_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FullDescription\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10910/18338336.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(model, sample, col_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                    for i, tok in enumerate(sample_col_tokens)) \n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;34m*\u001b[0m\u001b[0mpredictions_drop_one_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_drop_one_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredictions_drop_one_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_col_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/paulshab/NLP_env/nlp_environ/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/paulshab/NLP_env/nlp_environ/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/paulshab/NLP_env/nlp_environ/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not dict"
     ]
    }
   ],
   "source": [
    "i = 12077\n",
    "tokens_and_weights = explain(model, data.loc[i], \"Title\")\n",
    "draw_html([(tok, weight * 5) for tok, weight in tokens_and_weights], font_style='font-size:20px;');\n",
    "\n",
    "tokens_and_weights = explain(model, data.loc[i], \"FullDescription\")\n",
    "draw_html([(tok, weight * 10) for tok, weight in tokens_and_weights]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1v6ewkN7HgN"
   },
   "outputs": [],
   "source": [
    "i = np.random.randint(len(data))\n",
    "print(\"Index:\", i)\n",
    "print(\"Salary (gbp):\", np.expm1(model.predict(make_batch(data.iloc[i: i+1]))[0, 0]))\n",
    "\n",
    "tokens_and_weights = explain(model, data.loc[i], \"Title\")\n",
    "draw_html([(tok, weight * 5) for tok, weight in tokens_and_weights], font_style='font-size:20px;');\n",
    "\n",
    "tokens_and_weights = explain(model, data.loc[i], \"FullDescription\")\n",
    "draw_html([(tok, weight * 10) for tok, weight in tokens_and_weights]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Fr421tw7HgN"
   },
   "source": [
    "__Terrible start-up idea #1962:__ make a tool that automaticaly rephrases your job description (or CV) to meet salary expectations :)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
